{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): ...working... done\n",
      "Solving environment: ...working... done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: C:\\Users\\Harry\\anaconda3\n",
      "\n",
      "  added / updated specs:\n",
      "    - imbalanced-learn\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    conda-22.9.0               |   py39hcbf5309_2         985 KB  conda-forge\n",
      "    imbalanced-learn-0.7.0     |             py_1          97 KB  conda-forge\n",
      "    python_abi-3.9             |           2_cp39           4 KB  conda-forge\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:         1.1 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  imbalanced-learn   conda-forge/noarch::imbalanced-learn-0.7.0-py_1 None\n",
      "  python_abi         conda-forge/win-64::python_abi-3.9-2_cp39 None\n",
      "\n",
      "The following packages will be UPDATED:\n",
      "\n",
      "  conda              pkgs/main::conda-22.9.0-py39haa95532_0 --> conda-forge::conda-22.9.0-py39hcbf5309_2 None\n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "\n",
      "imbalanced-learn-0.7 | 97 KB     |            |   0% \n",
      "imbalanced-learn-0.7 | 97 KB     | #6         |  17% \n",
      "imbalanced-learn-0.7 | 97 KB     | ########## | 100% \n",
      "imbalanced-learn-0.7 | 97 KB     | ########## | 100% \n",
      "\n",
      "conda-22.9.0         | 985 KB    |            |   0% \n",
      "conda-22.9.0         | 985 KB    | ########## | 100% \n",
      "conda-22.9.0         | 985 KB    | ########## | 100% \n",
      "\n",
      "python_abi-3.9       | 4 KB      |            |   0% \n",
      "python_abi-3.9       | 4 KB      | ########## | 100% \n",
      "Preparing transaction: ...working... done\n",
      "Verifying transaction: ...working... done\n",
      "Executing transaction: ...working... done\n",
      "Retrieving notices: ...working... done\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "conda install -c conda-forge imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary modules\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"creditcard.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "\n",
       "         V8        V9  ...       V21       V22       V23       V24       V25  \\\n",
       "0  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928  0.128539   \n",
       "1  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846  0.167170   \n",
       "2  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281 -0.327642   \n",
       "3  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575  0.647376   \n",
       "4 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458  0.141267 -0.206010   \n",
       "\n",
       "        V26       V27       V28  Amount  Class  \n",
       "0 -0.189115  0.133558 -0.021053  149.62      0  \n",
       "1  0.125895 -0.008983  0.014724    2.69      0  \n",
       "2 -0.139097 -0.055353 -0.059752  378.66      0  \n",
       "3 -0.221929  0.062723  0.061458  123.50      0  \n",
       "4  0.502292  0.219422  0.215153   69.99      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    284315\n",
       "1       492\n",
       "Name: Class, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Harry\\anaconda3\\lib\\site-packages\\seaborn\\_decorators.py:36: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='Class', ylabel='count'>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(data['Class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Time      0\n",
       "V1        0\n",
       "V2        0\n",
       "V3        0\n",
       "V4        0\n",
       "V5        0\n",
       "V6        0\n",
       "V7        0\n",
       "V8        0\n",
       "V9        0\n",
       "V10       0\n",
       "V11       0\n",
       "V12       0\n",
       "V13       0\n",
       "V14       0\n",
       "V15       0\n",
       "V16       0\n",
       "V17       0\n",
       "V18       0\n",
       "V19       0\n",
       "V20       0\n",
       "V21       0\n",
       "V22       0\n",
       "V23       0\n",
       "V24       0\n",
       "V25       0\n",
       "V26       0\n",
       "V27       0\n",
       "V28       0\n",
       "Amount    0\n",
       "Class     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hence there are no mission values in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = data.drop(['Time'],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>0.090794</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>-0.166974</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>0.207643</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>-0.054952</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>0.753074</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284802</th>\n",
       "      <td>-11.881118</td>\n",
       "      <td>10.071785</td>\n",
       "      <td>-9.834783</td>\n",
       "      <td>-2.066656</td>\n",
       "      <td>-5.364473</td>\n",
       "      <td>-2.606837</td>\n",
       "      <td>-4.918215</td>\n",
       "      <td>7.305334</td>\n",
       "      <td>1.914428</td>\n",
       "      <td>4.356170</td>\n",
       "      <td>...</td>\n",
       "      <td>0.213454</td>\n",
       "      <td>0.111864</td>\n",
       "      <td>1.014480</td>\n",
       "      <td>-0.509348</td>\n",
       "      <td>1.436807</td>\n",
       "      <td>0.250034</td>\n",
       "      <td>0.943651</td>\n",
       "      <td>0.823731</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284803</th>\n",
       "      <td>-0.732789</td>\n",
       "      <td>-0.055080</td>\n",
       "      <td>2.035030</td>\n",
       "      <td>-0.738589</td>\n",
       "      <td>0.868229</td>\n",
       "      <td>1.058415</td>\n",
       "      <td>0.024330</td>\n",
       "      <td>0.294869</td>\n",
       "      <td>0.584800</td>\n",
       "      <td>-0.975926</td>\n",
       "      <td>...</td>\n",
       "      <td>0.214205</td>\n",
       "      <td>0.924384</td>\n",
       "      <td>0.012463</td>\n",
       "      <td>-1.016226</td>\n",
       "      <td>-0.606624</td>\n",
       "      <td>-0.395255</td>\n",
       "      <td>0.068472</td>\n",
       "      <td>-0.053527</td>\n",
       "      <td>24.79</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284804</th>\n",
       "      <td>1.919565</td>\n",
       "      <td>-0.301254</td>\n",
       "      <td>-3.249640</td>\n",
       "      <td>-0.557828</td>\n",
       "      <td>2.630515</td>\n",
       "      <td>3.031260</td>\n",
       "      <td>-0.296827</td>\n",
       "      <td>0.708417</td>\n",
       "      <td>0.432454</td>\n",
       "      <td>-0.484782</td>\n",
       "      <td>...</td>\n",
       "      <td>0.232045</td>\n",
       "      <td>0.578229</td>\n",
       "      <td>-0.037501</td>\n",
       "      <td>0.640134</td>\n",
       "      <td>0.265745</td>\n",
       "      <td>-0.087371</td>\n",
       "      <td>0.004455</td>\n",
       "      <td>-0.026561</td>\n",
       "      <td>67.88</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284805</th>\n",
       "      <td>-0.240440</td>\n",
       "      <td>0.530483</td>\n",
       "      <td>0.702510</td>\n",
       "      <td>0.689799</td>\n",
       "      <td>-0.377961</td>\n",
       "      <td>0.623708</td>\n",
       "      <td>-0.686180</td>\n",
       "      <td>0.679145</td>\n",
       "      <td>0.392087</td>\n",
       "      <td>-0.399126</td>\n",
       "      <td>...</td>\n",
       "      <td>0.265245</td>\n",
       "      <td>0.800049</td>\n",
       "      <td>-0.163298</td>\n",
       "      <td>0.123205</td>\n",
       "      <td>-0.569159</td>\n",
       "      <td>0.546668</td>\n",
       "      <td>0.108821</td>\n",
       "      <td>0.104533</td>\n",
       "      <td>10.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284806</th>\n",
       "      <td>-0.533413</td>\n",
       "      <td>-0.189733</td>\n",
       "      <td>0.703337</td>\n",
       "      <td>-0.506271</td>\n",
       "      <td>-0.012546</td>\n",
       "      <td>-0.649617</td>\n",
       "      <td>1.577006</td>\n",
       "      <td>-0.414650</td>\n",
       "      <td>0.486180</td>\n",
       "      <td>-0.915427</td>\n",
       "      <td>...</td>\n",
       "      <td>0.261057</td>\n",
       "      <td>0.643078</td>\n",
       "      <td>0.376777</td>\n",
       "      <td>0.008797</td>\n",
       "      <td>-0.473649</td>\n",
       "      <td>-0.818267</td>\n",
       "      <td>-0.002415</td>\n",
       "      <td>0.013649</td>\n",
       "      <td>217.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>284807 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               V1         V2        V3        V4        V5        V6  \\\n",
       "0       -1.359807  -0.072781  2.536347  1.378155 -0.338321  0.462388   \n",
       "1        1.191857   0.266151  0.166480  0.448154  0.060018 -0.082361   \n",
       "2       -1.358354  -1.340163  1.773209  0.379780 -0.503198  1.800499   \n",
       "3       -0.966272  -0.185226  1.792993 -0.863291 -0.010309  1.247203   \n",
       "4       -1.158233   0.877737  1.548718  0.403034 -0.407193  0.095921   \n",
       "...           ...        ...       ...       ...       ...       ...   \n",
       "284802 -11.881118  10.071785 -9.834783 -2.066656 -5.364473 -2.606837   \n",
       "284803  -0.732789  -0.055080  2.035030 -0.738589  0.868229  1.058415   \n",
       "284804   1.919565  -0.301254 -3.249640 -0.557828  2.630515  3.031260   \n",
       "284805  -0.240440   0.530483  0.702510  0.689799 -0.377961  0.623708   \n",
       "284806  -0.533413  -0.189733  0.703337 -0.506271 -0.012546 -0.649617   \n",
       "\n",
       "              V7        V8        V9       V10  ...       V21       V22  \\\n",
       "0       0.239599  0.098698  0.363787  0.090794  ... -0.018307  0.277838   \n",
       "1      -0.078803  0.085102 -0.255425 -0.166974  ... -0.225775 -0.638672   \n",
       "2       0.791461  0.247676 -1.514654  0.207643  ...  0.247998  0.771679   \n",
       "3       0.237609  0.377436 -1.387024 -0.054952  ... -0.108300  0.005274   \n",
       "4       0.592941 -0.270533  0.817739  0.753074  ... -0.009431  0.798278   \n",
       "...          ...       ...       ...       ...  ...       ...       ...   \n",
       "284802 -4.918215  7.305334  1.914428  4.356170  ...  0.213454  0.111864   \n",
       "284803  0.024330  0.294869  0.584800 -0.975926  ...  0.214205  0.924384   \n",
       "284804 -0.296827  0.708417  0.432454 -0.484782  ...  0.232045  0.578229   \n",
       "284805 -0.686180  0.679145  0.392087 -0.399126  ...  0.265245  0.800049   \n",
       "284806  1.577006 -0.414650  0.486180 -0.915427  ...  0.261057  0.643078   \n",
       "\n",
       "             V23       V24       V25       V26       V27       V28  Amount  \\\n",
       "0      -0.110474  0.066928  0.128539 -0.189115  0.133558 -0.021053  149.62   \n",
       "1       0.101288 -0.339846  0.167170  0.125895 -0.008983  0.014724    2.69   \n",
       "2       0.909412 -0.689281 -0.327642 -0.139097 -0.055353 -0.059752  378.66   \n",
       "3      -0.190321 -1.175575  0.647376 -0.221929  0.062723  0.061458  123.50   \n",
       "4      -0.137458  0.141267 -0.206010  0.502292  0.219422  0.215153   69.99   \n",
       "...          ...       ...       ...       ...       ...       ...     ...   \n",
       "284802  1.014480 -0.509348  1.436807  0.250034  0.943651  0.823731    0.77   \n",
       "284803  0.012463 -1.016226 -0.606624 -0.395255  0.068472 -0.053527   24.79   \n",
       "284804 -0.037501  0.640134  0.265745 -0.087371  0.004455 -0.026561   67.88   \n",
       "284805 -0.163298  0.123205 -0.569159  0.546668  0.108821  0.104533   10.00   \n",
       "284806  0.376777  0.008797 -0.473649 -0.818267 -0.002415  0.013649  217.00   \n",
       "\n",
       "        Class  \n",
       "0           0  \n",
       "1           0  \n",
       "2           0  \n",
       "3           0  \n",
       "4           0  \n",
       "...       ...  \n",
       "284802      0  \n",
       "284803      0  \n",
       "284804      0  \n",
       "284805      0  \n",
       "284806      0  \n",
       "\n",
       "[284807 rows x 30 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    284315\n",
       "1       492\n",
       "Name: Class, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "# normalise the amount column\n",
    "data['normAmount'] = StandardScaler().fit_transform(np.array(data['Amount']).reshape(-1, 1))\n",
    "\n",
    "# drop Time and Amount columns as they are not relevant for prediction purpose\n",
    "data1 = data.drop(['Time', 'Amount'], axis = 1)\n",
    "\n",
    "# as you can see there are 492 fraud transactions.\n",
    "data['Class'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Class</th>\n",
       "      <th>normAmount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>0.090794</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>0</td>\n",
       "      <td>0.244964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>-0.166974</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.342475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>0.207643</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>0</td>\n",
       "      <td>1.160686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>-0.054952</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>0</td>\n",
       "      <td>0.140534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>0.753074</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.073403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284802</th>\n",
       "      <td>-11.881118</td>\n",
       "      <td>10.071785</td>\n",
       "      <td>-9.834783</td>\n",
       "      <td>-2.066656</td>\n",
       "      <td>-5.364473</td>\n",
       "      <td>-2.606837</td>\n",
       "      <td>-4.918215</td>\n",
       "      <td>7.305334</td>\n",
       "      <td>1.914428</td>\n",
       "      <td>4.356170</td>\n",
       "      <td>...</td>\n",
       "      <td>0.213454</td>\n",
       "      <td>0.111864</td>\n",
       "      <td>1.014480</td>\n",
       "      <td>-0.509348</td>\n",
       "      <td>1.436807</td>\n",
       "      <td>0.250034</td>\n",
       "      <td>0.943651</td>\n",
       "      <td>0.823731</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.350151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284803</th>\n",
       "      <td>-0.732789</td>\n",
       "      <td>-0.055080</td>\n",
       "      <td>2.035030</td>\n",
       "      <td>-0.738589</td>\n",
       "      <td>0.868229</td>\n",
       "      <td>1.058415</td>\n",
       "      <td>0.024330</td>\n",
       "      <td>0.294869</td>\n",
       "      <td>0.584800</td>\n",
       "      <td>-0.975926</td>\n",
       "      <td>...</td>\n",
       "      <td>0.214205</td>\n",
       "      <td>0.924384</td>\n",
       "      <td>0.012463</td>\n",
       "      <td>-1.016226</td>\n",
       "      <td>-0.606624</td>\n",
       "      <td>-0.395255</td>\n",
       "      <td>0.068472</td>\n",
       "      <td>-0.053527</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.254117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284804</th>\n",
       "      <td>1.919565</td>\n",
       "      <td>-0.301254</td>\n",
       "      <td>-3.249640</td>\n",
       "      <td>-0.557828</td>\n",
       "      <td>2.630515</td>\n",
       "      <td>3.031260</td>\n",
       "      <td>-0.296827</td>\n",
       "      <td>0.708417</td>\n",
       "      <td>0.432454</td>\n",
       "      <td>-0.484782</td>\n",
       "      <td>...</td>\n",
       "      <td>0.232045</td>\n",
       "      <td>0.578229</td>\n",
       "      <td>-0.037501</td>\n",
       "      <td>0.640134</td>\n",
       "      <td>0.265745</td>\n",
       "      <td>-0.087371</td>\n",
       "      <td>0.004455</td>\n",
       "      <td>-0.026561</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.081839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284805</th>\n",
       "      <td>-0.240440</td>\n",
       "      <td>0.530483</td>\n",
       "      <td>0.702510</td>\n",
       "      <td>0.689799</td>\n",
       "      <td>-0.377961</td>\n",
       "      <td>0.623708</td>\n",
       "      <td>-0.686180</td>\n",
       "      <td>0.679145</td>\n",
       "      <td>0.392087</td>\n",
       "      <td>-0.399126</td>\n",
       "      <td>...</td>\n",
       "      <td>0.265245</td>\n",
       "      <td>0.800049</td>\n",
       "      <td>-0.163298</td>\n",
       "      <td>0.123205</td>\n",
       "      <td>-0.569159</td>\n",
       "      <td>0.546668</td>\n",
       "      <td>0.108821</td>\n",
       "      <td>0.104533</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.313249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284806</th>\n",
       "      <td>-0.533413</td>\n",
       "      <td>-0.189733</td>\n",
       "      <td>0.703337</td>\n",
       "      <td>-0.506271</td>\n",
       "      <td>-0.012546</td>\n",
       "      <td>-0.649617</td>\n",
       "      <td>1.577006</td>\n",
       "      <td>-0.414650</td>\n",
       "      <td>0.486180</td>\n",
       "      <td>-0.915427</td>\n",
       "      <td>...</td>\n",
       "      <td>0.261057</td>\n",
       "      <td>0.643078</td>\n",
       "      <td>0.376777</td>\n",
       "      <td>0.008797</td>\n",
       "      <td>-0.473649</td>\n",
       "      <td>-0.818267</td>\n",
       "      <td>-0.002415</td>\n",
       "      <td>0.013649</td>\n",
       "      <td>0</td>\n",
       "      <td>0.514355</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>284807 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               V1         V2        V3        V4        V5        V6  \\\n",
       "0       -1.359807  -0.072781  2.536347  1.378155 -0.338321  0.462388   \n",
       "1        1.191857   0.266151  0.166480  0.448154  0.060018 -0.082361   \n",
       "2       -1.358354  -1.340163  1.773209  0.379780 -0.503198  1.800499   \n",
       "3       -0.966272  -0.185226  1.792993 -0.863291 -0.010309  1.247203   \n",
       "4       -1.158233   0.877737  1.548718  0.403034 -0.407193  0.095921   \n",
       "...           ...        ...       ...       ...       ...       ...   \n",
       "284802 -11.881118  10.071785 -9.834783 -2.066656 -5.364473 -2.606837   \n",
       "284803  -0.732789  -0.055080  2.035030 -0.738589  0.868229  1.058415   \n",
       "284804   1.919565  -0.301254 -3.249640 -0.557828  2.630515  3.031260   \n",
       "284805  -0.240440   0.530483  0.702510  0.689799 -0.377961  0.623708   \n",
       "284806  -0.533413  -0.189733  0.703337 -0.506271 -0.012546 -0.649617   \n",
       "\n",
       "              V7        V8        V9       V10  ...       V21       V22  \\\n",
       "0       0.239599  0.098698  0.363787  0.090794  ... -0.018307  0.277838   \n",
       "1      -0.078803  0.085102 -0.255425 -0.166974  ... -0.225775 -0.638672   \n",
       "2       0.791461  0.247676 -1.514654  0.207643  ...  0.247998  0.771679   \n",
       "3       0.237609  0.377436 -1.387024 -0.054952  ... -0.108300  0.005274   \n",
       "4       0.592941 -0.270533  0.817739  0.753074  ... -0.009431  0.798278   \n",
       "...          ...       ...       ...       ...  ...       ...       ...   \n",
       "284802 -4.918215  7.305334  1.914428  4.356170  ...  0.213454  0.111864   \n",
       "284803  0.024330  0.294869  0.584800 -0.975926  ...  0.214205  0.924384   \n",
       "284804 -0.296827  0.708417  0.432454 -0.484782  ...  0.232045  0.578229   \n",
       "284805 -0.686180  0.679145  0.392087 -0.399126  ...  0.265245  0.800049   \n",
       "284806  1.577006 -0.414650  0.486180 -0.915427  ...  0.261057  0.643078   \n",
       "\n",
       "             V23       V24       V25       V26       V27       V28  Class  \\\n",
       "0      -0.110474  0.066928  0.128539 -0.189115  0.133558 -0.021053      0   \n",
       "1       0.101288 -0.339846  0.167170  0.125895 -0.008983  0.014724      0   \n",
       "2       0.909412 -0.689281 -0.327642 -0.139097 -0.055353 -0.059752      0   \n",
       "3      -0.190321 -1.175575  0.647376 -0.221929  0.062723  0.061458      0   \n",
       "4      -0.137458  0.141267 -0.206010  0.502292  0.219422  0.215153      0   \n",
       "...          ...       ...       ...       ...       ...       ...    ...   \n",
       "284802  1.014480 -0.509348  1.436807  0.250034  0.943651  0.823731      0   \n",
       "284803  0.012463 -1.016226 -0.606624 -0.395255  0.068472 -0.053527      0   \n",
       "284804 -0.037501  0.640134  0.265745 -0.087371  0.004455 -0.026561      0   \n",
       "284805 -0.163298  0.123205 -0.569159  0.546668  0.108821  0.104533      0   \n",
       "284806  0.376777  0.008797 -0.473649 -0.818267 -0.002415  0.013649      0   \n",
       "\n",
       "        normAmount  \n",
       "0         0.244964  \n",
       "1        -0.342475  \n",
       "2         1.160686  \n",
       "3         0.140534  \n",
       "4        -0.073403  \n",
       "...            ...  \n",
       "284802   -0.350151  \n",
       "284803   -0.254117  \n",
       "284804   -0.081839  \n",
       "284805   -0.313249  \n",
       "284806    0.514355  \n",
       "\n",
       "[284807 rows x 30 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         0\n",
       "1         0\n",
       "2         0\n",
       "3         0\n",
       "4         0\n",
       "         ..\n",
       "284802    0\n",
       "284803    0\n",
       "284804    0\n",
       "284805    0\n",
       "284806    0\n",
       "Name: Class, Length: 284807, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X = data1.iloc[:,0:28]\n",
    "\n",
    "X['normAmount'] = data1['normAmount']\n",
    "X\n",
    "y = data1.iloc[:,28]\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number transactions X_train dataset:  (199364, 29)\n",
      "Number transactions y_train dataset:  (199364,)\n",
      "Number transactions X_test dataset:  (85443, 29)\n",
      "Number transactions y_test dataset:  (85443,)\n"
     ]
    }
   ],
   "source": [
    "# split into 70:30 ration\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)\n",
    "\n",
    "# describes info about train and test set\n",
    "print(\"Number transactions X_train dataset: \", X_train.shape)\n",
    "print(\"Number transactions y_train dataset: \", y_train.shape)\n",
    "print(\"Number transactions X_test dataset: \", X_test.shape)\n",
    "print(\"Number transactions y_test dataset: \", y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     85296\n",
      "           1       0.88      0.62      0.73       147\n",
      "\n",
      "    accuracy                           1.00     85443\n",
      "   macro avg       0.94      0.81      0.86     85443\n",
      "weighted avg       1.00      1.00      1.00     85443\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# logistic regression object\n",
    "lr = LogisticRegression()\n",
    "\n",
    "# train the model on train set\n",
    "lr.fit(X_train, y_train.ravel())\n",
    "\n",
    "predictions = lr.predict(X_test)\n",
    "\n",
    "# print classification report\n",
    "print(classification_report(y_test, predictions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oversampling the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before OverSampling, counts of label '1': 345\n",
      "Before OverSampling, counts of label '0': 199019 \n",
      "\n",
      "After OverSampling, the shape of train_X: (398038, 29)\n",
      "After OverSampling, the shape of train_y: (398038,) \n",
      "\n",
      "After OverSampling, counts of label '1': 199019\n",
      "After OverSampling, counts of label '0': 199019\n"
     ]
    }
   ],
   "source": [
    "print(\"Before OverSampling, counts of label '1': {}\".format(sum(y_train == 1)))\n",
    "print(\"Before OverSampling, counts of label '0': {} \\n\".format(sum(y_train == 0)))\n",
    "\n",
    "# import SMOTE module from imblearn library\n",
    "# pip install imblearn (if you don't have imblearn in your system)\n",
    "from imblearn.over_sampling import SMOTE\n",
    "sm = SMOTE(random_state = 2)\n",
    "X_train_res, y_train_res = sm.fit_sample(X_train, y_train.ravel())\n",
    "\n",
    "print('After OverSampling, the shape of train_X: {}'.format(X_train_res.shape))\n",
    "print('After OverSampling, the shape of train_y: {} \\n'.format(y_train_res.shape))\n",
    "\n",
    "print(\"After OverSampling, counts of label '1': {}\".format(sum(y_train_res == 1)))\n",
    "print(\"After OverSampling, counts of label '0': {}\".format(sum(y_train_res == 0)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Under Sampling the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Undersampling, counts of label '1': 345\n",
      "Before Undersampling, counts of label '0': 199019 \n",
      "\n",
      "After Undersampling, the shape of train_X: (690, 29)\n",
      "After Undersampling, the shape of train_y: (690,) \n",
      "\n",
      "After Undersampling, counts of label '1': 345\n",
      "After Undersampling, counts of label '0': 345\n"
     ]
    }
   ],
   "source": [
    "print(\"Before Undersampling, counts of label '1': {}\".format(sum(y_train == 1)))\n",
    "print(\"Before Undersampling, counts of label '0': {} \\n\".format(sum(y_train == 0)))\n",
    "\n",
    "# apply near miss\n",
    "from imblearn.under_sampling import NearMiss\n",
    "nr = NearMiss()\n",
    "\n",
    "X_train_miss, y_train_miss = nr.fit_sample(X_train, y_train.ravel())\n",
    "\n",
    "print('After Undersampling, the shape of train_X: {}'.format(X_train_miss.shape))\n",
    "print('After Undersampling, the shape of train_y: {} \\n'.format(y_train_miss.shape))\n",
    "\n",
    "print(\"After Undersampling, counts of label '1': {}\".format(sum(y_train_miss == 1)))\n",
    "print(\"After Undersampling, counts of label '0': {}\".format(sum(y_train_miss == 0)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99     85296\n",
      "           1       0.06      0.92      0.11       147\n",
      "\n",
      "    accuracy                           0.98     85443\n",
      "   macro avg       0.53      0.95      0.55     85443\n",
      "weighted avg       1.00      0.98      0.99     85443\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr1 = LogisticRegression()\n",
    "lr1.fit(X_train_res, y_train_res.ravel())\n",
    "predictions = lr1.predict(X_test)\n",
    "  \n",
    "# print classification report\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For under Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.55      0.71     85296\n",
      "           1       0.00      0.95      0.01       147\n",
      "\n",
      "    accuracy                           0.56     85443\n",
      "   macro avg       0.50      0.75      0.36     85443\n",
      "weighted avg       1.00      0.56      0.71     85443\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# train the model on train set\n",
    "lr2 = LogisticRegression()\n",
    "lr2.fit(X_train_miss, y_train_miss.ravel())\n",
    "predictions = lr2.predict(X_test)\n",
    "  \n",
    "# print classification report\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Harry\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     85296\n",
      "           1       0.50      0.86      0.64       147\n",
      "\n",
      "    accuracy                           1.00     85443\n",
      "   macro avg       0.75      0.93      0.82     85443\n",
      "weighted avg       1.00      1.00      1.00     85443\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "KNN = KNeighborsClassifier()\n",
    "KNN.fit(X_train_res, y_train_res.ravel())\n",
    "predictions = KNN.predict(X_test)\n",
    "\n",
    "# print classification report\n",
    "print(classification_report(y_test, predictions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Harry\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.73      0.84     85296\n",
      "           1       0.01      0.93      0.01       147\n",
      "\n",
      "    accuracy                           0.73     85443\n",
      "   macro avg       0.50      0.83      0.43     85443\n",
      "weighted avg       1.00      0.73      0.84     85443\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# train the model on train set\n",
    "KNN1 = KNeighborsClassifier()\n",
    "KNN1.fit(X_train_miss, y_train_miss.ravel())\n",
    "predictions_1 = KNN1.predict(X_test)\n",
    "  \n",
    "# print classification report\n",
    "print(classification_report(y_test, predictions_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      0.99     85296\n",
      "           1       0.09      0.86      0.17       147\n",
      "\n",
      "    accuracy                           0.99     85443\n",
      "   macro avg       0.55      0.92      0.58     85443\n",
      "weighted avg       1.00      0.99      0.99     85443\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "SVM = svm.SVC(kernel='rbf') \n",
    "\n",
    "SVM.fit(X_train_res, y_train_res.ravel())\n",
    "predictions = SVM.predict(X_test)\n",
    "\n",
    "# print classification report\n",
    "print(classification_report(y_test, predictions))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.86      0.93     85296\n",
      "           1       0.01      0.88      0.02       147\n",
      "\n",
      "    accuracy                           0.86     85443\n",
      "   macro avg       0.51      0.87      0.47     85443\n",
      "weighted avg       1.00      0.86      0.92     85443\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# train the model on train set\n",
    "SVM1 = svm.SVC(kernel = 'rbf')\n",
    "SVM1.fit(X_train_miss, y_train_miss.ravel())\n",
    "predictions_1 = SVM1.predict(X_test)\n",
    "  \n",
    "# print classification report\n",
    "print(classification_report(y_test, predictions_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying Xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): ...working... done\n",
      "Solving environment: ...working... done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: C:\\Users\\Harry\\anaconda3\n",
      "\n",
      "  added / updated specs:\n",
      "    - xgboost\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    _py-xgboost-mutex-2.0      |            cpu_0          11 KB  conda-forge\n",
      "    libxgboost-1.5.0           |       hd77b12b_2         1.3 MB\n",
      "    py-xgboost-1.5.0           |   py39haa95532_2         156 KB\n",
      "    xgboost-1.5.0              |   py39haa95532_2          15 KB\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:         1.5 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  _py-xgboost-mutex  conda-forge/win-64::_py-xgboost-mutex-2.0-cpu_0 None\n",
      "  libxgboost         pkgs/main/win-64::libxgboost-1.5.0-hd77b12b_2 None\n",
      "  py-xgboost         pkgs/main/win-64::py-xgboost-1.5.0-py39haa95532_2 None\n",
      "  xgboost            pkgs/main/win-64::xgboost-1.5.0-py39haa95532_2 None\n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "\n",
      "_py-xgboost-mutex-2. | 11 KB     |            |   0% \n",
      "_py-xgboost-mutex-2. | 11 KB     | ########## | 100% \n",
      "_py-xgboost-mutex-2. | 11 KB     | ########## | 100% \n",
      "\n",
      "xgboost-1.5.0        | 15 KB     |            |   0% \n",
      "xgboost-1.5.0        | 15 KB     | ########## | 100% \n",
      "xgboost-1.5.0        | 15 KB     | ########## | 100% \n",
      "\n",
      "py-xgboost-1.5.0     | 156 KB    |            |   0% \n",
      "py-xgboost-1.5.0     | 156 KB    | ########## | 100% \n",
      "py-xgboost-1.5.0     | 156 KB    | ########## | 100% \n",
      "\n",
      "libxgboost-1.5.0     | 1.3 MB    |            |   0% \n",
      "libxgboost-1.5.0     | 1.3 MB    | #####5     |  55% \n",
      "libxgboost-1.5.0     | 1.3 MB    | ########## | 100% \n",
      "Preparing transaction: ...working... done\n",
      "Verifying transaction: ...working... done\n",
      "Executing transaction: ...working... done\n",
      "Retrieving notices: ...working... done\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "conda install -c conda-forge xgboost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): ...working... done\n",
      "Solving environment: ...working... done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n",
      "Retrieving notices: ...working... done\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "conda install -c anaconda py-xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Harry\\anaconda3\\lib\\site-packages\\xgboost\\compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    " from sklearn.ensemble import RandomForestClassifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      1.00     85296\n",
      "           1       0.19      0.85      0.31       147\n",
      "\n",
      "    accuracy                           0.99     85443\n",
      "   macro avg       0.59      0.92      0.65     85443\n",
      "weighted avg       1.00      0.99      1.00     85443\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "RFC = RandomForestClassifier(max_depth=2, random_state=0)\n",
    "\n",
    "RFC.fit(X_train_res, y_train_res.ravel())\n",
    "predictions = RFC.predict(X_test)\n",
    "\n",
    "# print classification report\n",
    "print(classification_report(y_test, predictions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.47      0.64     85296\n",
      "           1       0.00      0.94      0.01       147\n",
      "\n",
      "    accuracy                           0.47     85443\n",
      "   macro avg       0.50      0.70      0.32     85443\n",
      "weighted avg       1.00      0.47      0.64     85443\n",
      "\n"
     ]
    }
   ],
   "source": [
    "RFC1 = RandomForestClassifier(max_depth=2, random_state=0)\n",
    "RFC1.fit(X_train_miss, y_train_miss.ravel())\n",
    "predictions_1 = RFC1.predict(X_test)\n",
    "  \n",
    "# print classification report\n",
    "print(classification_report(y_test, predictions_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Harry\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\Users\\Harry\\anaconda3\\lib\\site-packages\\xgboost\\data.py:250: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[02:11:05] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     85296\n",
      "           1       0.77      0.85      0.81       147\n",
      "\n",
      "    accuracy                           1.00     85443\n",
      "   macro avg       0.89      0.92      0.90     85443\n",
      "weighted avg       1.00      1.00      1.00     85443\n",
      "\n"
     ]
    }
   ],
   "source": [
    "regressor = xgb.XGBClassifier()\n",
    "   \n",
    "\n",
    "regressor.fit(X_train_res, y_train_res.ravel())\n",
    "predictions = regressor.predict(X_test)\n",
    "\n",
    "# print classification report\n",
    "print(classification_report(y_test, predictions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Harry\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\Users\\Harry\\anaconda3\\lib\\site-packages\\xgboost\\data.py:250: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[02:10:39] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.12      0.22     85296\n",
      "           1       0.00      0.97      0.00       147\n",
      "\n",
      "    accuracy                           0.12     85443\n",
      "   macro avg       0.50      0.55      0.11     85443\n",
      "weighted avg       1.00      0.12      0.22     85443\n",
      "\n"
     ]
    }
   ],
   "source": [
    "regressor1 = xgb.XGBClassifier()\n",
    "\n",
    "regressor1.fit(X_train_miss, y_train_miss.ravel())\n",
    "predictions_1 = regressor1.predict(X_test)\n",
    "  \n",
    "# print classification report\n",
    "print(classification_report(y_test, predictions_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying NB classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99     85296\n",
      "           1       0.06      0.86      0.11       147\n",
      "\n",
      "    accuracy                           0.98     85443\n",
      "   macro avg       0.53      0.92      0.55     85443\n",
      "weighted avg       1.00      0.98      0.99     85443\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gnb = GaussianNB()\n",
    "gnb.fit(X_train_res, y_train_res.ravel())\n",
    "predictions = gnb.predict(X_test)\n",
    "\n",
    "# print classification report\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.23      0.37     85296\n",
      "           1       0.00      0.95      0.00       147\n",
      "\n",
      "    accuracy                           0.23     85443\n",
      "   macro avg       0.50      0.59      0.19     85443\n",
      "weighted avg       1.00      0.23      0.37     85443\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gnb1 = GaussianNB()\n",
    "gnb1.fit(X_train_miss, y_train_miss.ravel())\n",
    "predictions_1 = gnb1.predict(X_test)\n",
    "  \n",
    "# print classification report\n",
    "print(classification_report(y_test, predictions_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
